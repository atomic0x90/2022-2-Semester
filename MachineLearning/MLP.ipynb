datasetPath = "./drive/MyDrive/dataset/"
parameterPath = "./drive/MyDrive/parameters/"

import torch
import torch.nn as nn
import torchvision.datasets as dataset
import torchvision.transforms as transform
from torch.utils.data import DataLoader
import numpy as np

# Training dataset 다운로드
cifar10_train = dataset.CIFAR10(root = "./", # 데이터셋이 저장된 위치
                            train = True,
                            transform = transform.ToTensor(),
                            download = True)
# Testing dataset 다운로드
cifar10_test = dataset.CIFAR10(root = "./",
                            train = False,
                            transform = transform.ToTensor(),
                            download = True)

class MLP(nn.Module):
  
  def __init__(self):
    super(MLP, self).__init__()

    # 신경망 파라미터 선언 (3072 x 1000, 1000 x 128, 128 x 10)
    self.fc1 = nn.Linear(in_features=3072,out_features=1000)
    self.fc2 = nn.Linear(in_features=1000,out_features=128)
    self.fc3 = nn.Linear(in_features=128,out_features=10)

    # Activation function 선언: nn.ReLU()
    self.relu = nn.ReLU()
  def forward(self, x):
    # 이미지 평탄화 수행: batch size x 3072
    x = x.view(-1,32*32*3)
    
    # FC layer와 activation function 추가 (Softmax 함수는 자동 적용되므로 호출하지 않아도 상관없음)
    y = self.relu(self.fc1(x))
    y = self.relu(self.fc2(y))
    y = self.fc3(y)
    return y

class LeNet5(nn.Module):
  
  def __init__(self):
    super(LeNet5, self).__init__()

    # 신경망 파라미터 선언 (3072 x 1000, 1000 x 128, 128 x 10)
    self.conv1 = nn.Conv2d(in_channels=3,out_channels=6,kernel_size=5)
    self.conv2 = nn.Conv2d(in_channels=6,out_channels=16,kernel_size=5)

    self.fc1 = nn.Linear(in_features=400,out_features=120)
    self.fc2 = nn.Linear(in_features=120,out_features=84)
    self.fc3 = nn.Linear(in_features=84,out_features=10)

    # Activation function 선언: nn.ReLU()
    self.relu = nn.ReLU()
    self.maxPool2d = nn.MaxPool2d(kernel_size=2,stride=2)
  def forward(self, x):
    # 이미지 평탄화 수행: batch size x 3072
    out = self.relu(self.conv1(x))
    out = self.maxPool2d(out)
    out = self.relu(self.conv2(out))
    out = self.maxPool2d(out)

    out = out.view(-1,400)
    out = self.relu(self.fc1(out))
    out = self.relu(self.fc2(out))
    out = self.fc3(out)
    return out
    
batch_size = 100
learning_rate = 0.1
training_epochs = 15
loss_function = nn.CrossEntropyLoss()
network = LeNet5()
optimizer = torch.optim.SGD(network.parameters(), lr = learning_rate)

data_loader = DataLoader(dataset=cifar10_train,
                         batch_size=batch_size,
                         shuffle=True,
                         drop_last=True)

for epoch in range(training_epochs):
  avg_cost = 0
  total_batch = len(data_loader)

  for img, label in data_loader:

    pred = network(img)

    loss = loss_function(pred, label)
    optimizer.zero_grad() # gradient 초기화
    loss.backward()
    optimizer.step()

    avg_cost += loss / total_batch
  
  print('Epoch: %d Loss = %f'%(epoch+1, avg_cost))

print('Learning finished')


network.eval()
img_test = torch.tensor(np.transpose(cifar10_test.data , (0, 3, 1, 2))) /255.
label_test = torch.tensor(cifar10_test.targets)

with torch.no_grad(): # test에서는 기울기 계산 제외
    prediction = network(img_test) # 전체 test data를 한번에 계산

correct_prediction = torch.argmax(prediction, 1) == label_test
accuracy = correct_prediction.float().mean()
print('Accuracy:', accuracy.item())

network = network.to('cuda:0')
for epoch in range(training_epochs):
  avg_cost = 0
  total_batch = len(data_loader)

  for img, label in data_loader:
    
    img = img.to('cuda:0')
    label = label.to('cuda:0')

    pred = network(img)

    loss = loss_function(pred, label)
    optimizer.zero_grad() # gradient 초기화
    loss.backward()
    optimizer.step()

    avg_cost += loss / total_batch
  
  print('Epoch: %d Loss = %f'%(epoch+1, avg_cost))

print('Learning finished')
